# WordEmbeddings avec Word2Vec

Ce projet explore l'utilisation des embeddings de mots, en particulier le modèle **Word2Vec**, pour la représentation des mots sous forme de vecteurs dans un espace vectoriel continu.
L'objectif est d'analyser la similarité sémantique entre les mots et de démontrer les capacités de ce modèle dans des tâches de traitement du langage naturel (NLP).

## Objectifs du Projet

L'objectif de ce projet est de :
- Créer un modèle **Word2Vec** pour générer des vecteurs de mots à partir d'un corpus de texte.
- Visualiser et analyser les relations sémantiques entre les mots.
- Comparer les performances des modèles pré-entraînés et des modèles entraînés sur des jeux de données plus petits.
- Utiliser **Gensim** et d'autres outils pour analyser les vecteurs de mots et leur similarité.

## Technologies et Outils Utilisés

- **Python** : Langage de programmation principal du projet.
- **Gensim** : Librairie utilisée pour entraîner et manipuler le modèle Word2Vec.
- **Scikit-learn** : Librairie pour la réduction de dimension (ACP).
- **Matplotlib** et **Seaborn** : Librairies de visualisation pour afficher les vecteurs de mots.
- **Jupyter Notebook** : Environnement interactif pour l'expérimentation et la visualisation des résultats.

Le projet suit plusieurs étapes, depuis l'entraînement d'un modèle Word2Vec sur un corpus jusqu'à l'utilisation de vecteurs pré-entraînés pour comparer des similarités entre mots.

